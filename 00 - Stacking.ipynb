{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00 - Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble é um conjunto de modelos de machine learning. Usamos mais de um tipo de modelo para fazer previsão, onde cada um influencia no resultado (podemos fazer uma média entre as previsões dos modelos do ensemble e dar assim o resultado final).\n",
    "\n",
    "Stacking é um método de Ensemble bem sucedido. No Stacking a ideia é usar as previsões de um modelo para treinar outro modelo. Por exemplo, podemos inicialmente aplicar os modelos de light gbm e uma rede neural aos dados, e depois aplicar uma modelo de regressão linear sobre as saídas desses dois modelos. A saída desses dois modelos originais são as features para treinamento do modelo de regressão linear.\n",
    "\n",
    "Dessa forma o modelo de regressão linear está sendo treinado em cima de uma transformação dos dados originais.\n",
    "\n",
    "As features originais são chamadas de nível zero, os dois modelos citados são o nível um e o modelo de regressão linear é o nível dois. Atentar que podemos ter mais níveis (quantos quisermos). Para isso basta começar com mais modelos no nível um (começar com três permite termos dois modelos no nível dois e um modelo em um nível três).\n",
    "\n",
    "O rácional para a escolha dos modelos que vão compor o stacking é mesclar grupos de modelos diferentes. Como grupos temos Random Forest, Lineares (RL, SVM, etc), GBM (ensemble de árvores) e Redes Neurais. Como exemplo, podemos fazer um GBM com uma rede neural. Esse é o mínimo. Junto podemos colocar uma regressão linear e um KNN.\n",
    "\n",
    "Para gerar mais diversidade nos dados resultantes dos modelos intermediários, podemos:\n",
    "\n",
    "1. Passar features especificas;\n",
    "2. Usar modelos difentes;\n",
    "3. Separa as linhas em conjuntos;\n",
    "4. Manipular os parametros dos modelos. Por exemplo, tendo dois modelos de mesmo tipo (exemplo GBM), mas cada um com parametros distintos permite a esses modelos se autocompletarem e captar detalhes nos dados que vão permitir os próximos modelos realizarem previsões mais acertivas e estáveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estamos lidando com dados de séries temporais, vamos usar a separação de passado e futuro.\n",
    "\n",
    "Como vamos lidar com 3 níveis, vamos separar o conjunto de dados de treino e 3 partes.\n",
    "\n",
    "Na primeira parte dos dados treinamos os modelos do nível 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib as jb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data-processed/train.csv\")\n",
    "\n",
    "\n",
    "# 40 eras de tempo para cada divisão dos dados de treino\n",
    "train_train_level0 = train[train['era'] < 41]\n",
    "train_train_level1 = train[(train['era'] >= 41) & (train['era'] < 81)] # train_level1 == valid_level_0\n",
    "train_valid_level1 = train[train['era'] >= 81]\n",
    "\n",
    "\n",
    "jb.dump(train_train_level0, \"data-processed/train_train_level0.pkl.z\")\n",
    "jb.dump(train_train_level1, \"data-processed/train_train_level1.pkl.z\")\n",
    "jb.dump(train_valid_level1, \"data-processed/train_valid_level1.pkl.z\")\n",
    "\n",
    "\n",
    "train_train_level0 = jb.load(\"data-processed/train_train_level0.pkl.z\")\n",
    "train_train_level1 = jb.load(\"data-processed/train_train_level1.pkl.z\")\n",
    "train_valid_level1 = jb.load(\"data-processed/train_valid_level1.pkl.z\")\n",
    "\n",
    "\n",
    "# Separação em X e Y\n",
    "# Bloco 1\n",
    "X_train0, y_train0 = train_train_level0.filter(regex=r'feature', axis=1), train_train_level0['target']\n",
    "# Bloco 2\n",
    "X_train1, y_train1 = train_train_level1.filter(regex=r'feature', axis=1), train_train_level1['target']\n",
    "# Bloco 3\n",
    "X_val1, y_val1 = train_valid_level1.filter(regex=r'feature', axis=1), train_valid_level1['target']\n",
    "\n",
    "\n",
    "# Dados de teste para o modelo final\n",
    "test = pd.read_csv(\"data-processed/test.csv\")\n",
    "X_test, y_test = test.filter(regex=r'feature', axis=1), test['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking: Nível 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking: Nível 0 - Criação do Primeiro Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from skopt import gp_minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**gp_minimize:**\n",
    "\n",
    "Otimização Bayesiana usando Processos Gaussianos.\n",
    "\n",
    "Se cada avaliação de função for cara, por exemplo, quando os parâmetros são os hiperparâmetros de uma rede neural e a avaliação da função é a pontuação média de validação cruzada em dez dobras, otimizar os hiperparâmetros por rotinas de otimização padrão demoraria uma eternidade!\n",
    "\n",
    "A ideia é aproximar a função usando um processo gaussiano. Em outras palavras, assume-se que os valores da função seguem uma gaussiana multivariada. A covariância dos valores da função é fornecida por um kernel GP entre os parâmetros. Então, uma escolha inteligente para escolher o próximo parâmetro a avaliar pode ser feita pela função de aquisição em vez do prior gaussiano, que é muito mais rápido de avaliar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_lgbm(params):\n",
    "    \n",
    "    num_leaves, min_data_in_leaf, learning_rate = params\n",
    "    \n",
    "    mdl = LGBMRegressor(num_leaves=num_leaves,\n",
    "                        min_data_in_leaf=min_data_in_leaf,\n",
    "                        learning_rate=learning_rate, \n",
    "                        n_estimators=100,\n",
    "                        random_state=0)\n",
    "    \n",
    "    # Treina o modelo no primeiro bloco de dados (note que só treina aqui, não tem mais treino abaixo)\n",
    "    mdl.fit(X_train0, y_train0)\n",
    "    \n",
    "    # Faz a previsão contra o segundo bloco de dados\n",
    "    p = mdl.predict(X_train1)\n",
    "    \n",
    "    # Salva o modelo treinado mais as previsões contra o segundo bloco de dados em disco\n",
    "    model_name_train1 = \"./preds_train1/lgbm_{}_{}_{}.pkl.z\".format(num_leaves, min_data_in_leaf, learning_rate) \n",
    "    jb.dump(p, model_name_train1)\n",
    "    \n",
    "    # Cálcula a métrica contra o segundo bloco de dados\n",
    "    metric = spearmanr(y_train1.values, p).correlation\n",
    "    \n",
    "    # Faz a previsão contra o terceiro bloco de dados\n",
    "    p = mdl.predict(X_val1)\n",
    "    \n",
    "    # Salva o modelo treinado mais as previsões contra o terceiro bloco de dados em disco\n",
    "    model_name_val1 = \"./preds_val1/lgbm_{}_{}_{}.pkl.z\".format(num_leaves, min_data_in_leaf, learning_rate) \n",
    "    jb.dump(p, model_name_val1)\n",
    "    \n",
    "    # Faz a previsão contra os dados de teste\n",
    "    p = mdl.predict(X_test)\n",
    "    \n",
    "    # Salva o modelo treinado mais as previsões contra os dados de teste em disco\n",
    "    model_name_test = \"./preds_test/lgbm_{}_{}_{}.pkl.z\".format(num_leaves, min_data_in_leaf, learning_rate) \n",
    "    jb.dump(p, model_name_test)\n",
    "    \n",
    "    print(params, metric)\n",
    "    print()\n",
    "    \n",
    "    return -metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hiperparâmetros mais importantes LGBMRegressor (na ordem de importância):\n",
    "\n",
    "1. learning rate\n",
    "2. num_leaves (ou max_depth, se for no xgboost)\n",
    "3. min_data_in_leaf\n",
    "4. subsample \n",
    "5. colsample (bytree ou bynode, não faz tanta diferença)\n",
    "\n",
    "learning_rate é 10x mais importante do que os hiperparâmetros 2 e 3, que são 100x mais importantes do que os hiperparâmetros 4 e 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=844, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=844\n",
      "[119, 844, 0.007210171877207321] 0.02311818303656751\n",
      "\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 25.9561\n",
      "Function value obtained: -0.0231\n",
      "Current minimum: -0.0231\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=624, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=624\n",
      "[170, 624, 0.002423157857285798] 0.016650656106097936\n",
      "\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 27.4248\n",
      "Function value obtained: -0.0167\n",
      "Current minimum: -0.0231\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=58, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=58\n",
      "[61, 58, 0.0018735112038624302] 0.014679138108802784\n",
      "\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 20.2670\n",
      "Function value obtained: -0.0147\n",
      "Current minimum: -0.0231\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=812, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=812\n",
      "[97, 812, 0.0030197929882137525] 0.016289496619978784\n",
      "\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 24.7698\n",
      "Function value obtained: -0.0163\n",
      "Current minimum: -0.0231\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=836, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=836\n",
      "[80, 836, 0.002174684006048089] 0.014630678957744036\n",
      "\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 23.7495\n",
      "Function value obtained: -0.0146\n",
      "Current minimum: -0.0231\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "[130, 369, 0.009060562467991657] 0.024357695471693478\n",
      "\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 26.6736\n",
      "Function value obtained: -0.0244\n",
      "Current minimum: -0.0244\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=870, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=870\n",
      "[30, 870, 0.0029758295038874645] 0.018977761887415925\n",
      "\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 18.7833\n",
      "Function value obtained: -0.0190\n",
      "Current minimum: -0.0244\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=521, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=521\n",
      "[161, 521, 0.004773968291550972] 0.021234217958219462\n",
      "\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 31.1366\n",
      "Function value obtained: -0.0212\n",
      "Current minimum: -0.0244\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=582, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=582\n",
      "[145, 582, 0.003446459897310035] 0.020126271602339842\n",
      "\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 24.2401\n",
      "Function value obtained: -0.0201\n",
      "Current minimum: -0.0244\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=107, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=107\n",
      "[152, 107, 0.0029757772507264775] 0.017753556535590263\n",
      "\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 28.2305\n",
      "Function value obtained: -0.0178\n",
      "Current minimum: -0.0244\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=910, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=910\n",
      "[24, 910, 0.01] 0.026105516329814785\n",
      "\n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 20.9133\n",
      "Function value obtained: -0.0261\n",
      "Current minimum: -0.0261\n"
     ]
    }
   ],
   "source": [
    "# Parâmetros para uso na função acima (vão para os hiperparâmetros do modelo LGBMRegressor)\n",
    "space = [(2, 200),\n",
    "         (1, 1000),\n",
    "         (1e-3, 1e-2, 'log-uniform')]\n",
    "\n",
    "# Chamada ao gp_minimize (n_calls é igual ao número de execuções do gp_minimize)\n",
    "res = gp_minimize(tune_lgbm, space, random_state=0, verbose=1, n_calls=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking: Nível 0 - Criação do Segundo Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação da rede neural\n",
    "\n",
    "class MLPRegressorTorch(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden1, hidden2):\n",
    "        super(MLPRegressorTorch,self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, hidden1)\n",
    "        self.hidden2 = nn.Linear(hidden1, hidden2)\n",
    "        self.out = nn.Linear(hidden2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        return self.out(x)\n",
    "    \n",
    "    \n",
    "#MLPRegressorTorch(X_train0.shape[1], 10, 10)(torch.from_numpy(X_train0.values).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste nos dados para serem entendidos pelo torch\n",
    "\n",
    "X_train0_t = torch.from_numpy(X_train0.values).float()\n",
    "y_train0_t = torch.from_numpy(y_train0.values).float().unsqueeze(dim=-1)\n",
    "\n",
    "X_train1_t = torch.from_numpy(X_train1.values).float()\n",
    "X_val1_t = torch.from_numpy(X_val1.values).float()\n",
    "X_test_t = torch.from_numpy(X_test.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_mlp(params):\n",
    "    \n",
    "    #scaling?\n",
    "    torch.manual_seed(0)\n",
    "    hidden1, hidden2, learning_rate = params\n",
    "    mdl = MLPRegressorTorch(X_train0.shape[1], hidden1, hidden2)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(mdl.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(100): \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        p = mdl(X_train0_t)\n",
    "        loss = criterion(p, y_train0_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    p = mdl(X_train1_t).detach().numpy()\n",
    "    model_name_train1 = \"./preds_train1/mlp_{}_{}_{}.pkl.z\".format(hidden1, hidden2, learning_rate) \n",
    "    jb.dump(p, model_name_train1)\n",
    "    #print(p)\n",
    "    \n",
    "    metric = spearmanr(y_train1.values,p).correlation\n",
    "    \n",
    "    p = mdl(X_val1_t).detach().numpy()\n",
    "    model_name_val1 = \"./preds_val1/mlp_{}_{}_{}.pkl.z\".format(hidden1, hidden2, learning_rate) \n",
    "    jb.dump(p, model_name_val1)\n",
    "    \n",
    "    p = mdl(X_test_t).detach().numpy()\n",
    "    model_name_test = \"./preds_test/mlp_{}_{}_{}.pkl.z\".format(hidden1, hidden2, learning_rate) \n",
    "    jb.dump(p, model_name_test)\n",
    "    \n",
    "    print(params, metric)\n",
    "    print()\n",
    "    \n",
    "    return -metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "[123, 170, 0.0005198657849887135] 0.007006575737804042\n",
      "\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 84.3772\n",
      "Function value obtained: -0.0070\n",
      "Current minimum: -0.0070\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "[171, 128, 5.871694001325899e-05] -0.0006740892450441023\n",
      "\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 124.1339\n",
      "Function value obtained: 0.0007\n",
      "Current minimum: -0.0070\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "[67, 21, 3.5100442309980525e-05] 0.0005255519051044248\n",
      "\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 37.1965\n",
      "Function value obtained: -0.0005\n",
      "Current minimum: -0.0070\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "[101, 164, 9.119149691664945e-05] 0.0035072046770885925\n",
      "\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 97.7884\n",
      "Function value obtained: -0.0035\n",
      "Current minimum: -0.0070\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "[85, 169, 4.729250526161365e-05] 0.0015514495737406768\n",
      "\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 103.9903\n",
      "Function value obtained: -0.0016\n",
      "Current minimum: -0.0070\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "[133, 80, 0.0008209379223637906] 0.014085941056679312\n",
      "\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 85.5896\n",
      "Function value obtained: -0.0141\n",
      "Current minimum: -0.0141\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "[37, 175, 8.855561236207113e-05] -0.002978384922624919\n",
      "\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 69.2845\n",
      "Function value obtained: 0.0030\n",
      "Current minimum: -0.0141\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "[162, 109, 0.00022790773248734106] 0.011054206627227852\n",
      "\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 112.1909\n",
      "Function value obtained: -0.0111\n",
      "Current minimum: -0.0141\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "[147, 121, 0.00011878085823766309] 0.005041001846120009\n",
      "\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 106.7674\n",
      "Function value obtained: -0.0050\n",
      "Current minimum: -0.0141\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "[154, 30, 8.855250245941234e-05] 0.004268436280360385\n",
      "\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 77.1639\n",
      "Function value obtained: -0.0043\n",
      "Current minimum: -0.0141\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "[200, 60, 0.001] 0.012257696619337333\n",
      "\n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 84.9824\n",
      "Function value obtained: -0.0123\n",
      "Current minimum: -0.0141\n"
     ]
    }
   ],
   "source": [
    "space = [(10, 200),\n",
    "         (10, 200),\n",
    "         (1e-5, 1e-3, 'log-uniform')]\n",
    "\n",
    "\n",
    "res = gp_minimize(tune_mlp, space, random_state=0, verbose=1, n_calls=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking: Nível 0 - Criação da Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178773, 22), (180938, 22))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega as previsões para o segundo e terceiro blocos de dados\n",
    "preds_train1 = glob.glob(\"./preds_train1/*.pkl.z\")\n",
    "preds_val1 = glob.glob(\"./preds_val1/*.pkl.z\")\n",
    "\n",
    "df_train1 = []\n",
    "df_val1 = []\n",
    "scores_val1 = dict()\n",
    "\n",
    "\n",
    "# Extrai todas as previsões realizadas contra o segundo bloco de dados\n",
    "for p_name in preds_train1:\n",
    "    p = jb.load(p_name)\n",
    "    p_df = pd.DataFrame(p, columns=[p_name])\n",
    "    df_train1.append(p_df)\n",
    "    \n",
    "\n",
    "# Extrai todas as previsões realizadas contra o terceiro bloco de dados\n",
    "for p_name in preds_val1:\n",
    "    p = jb.load(p_name)\n",
    "    p_df = pd.DataFrame(p, columns=[p_name])\n",
    "    df_val1.append(p_df)\n",
    "    scores_val1[p_name] = spearmanr(y_val1, p_df).correlation\n",
    "    \n",
    "\n",
    "# Assim temos os novos datasets, compostos só de previsões dos modelos anteriores\n",
    "df_train1 = pd.concat(df_train1, axis=1)\n",
    "df_val1 = pd.concat(df_val1, axis=1)\n",
    "\n",
    "\n",
    "# Shape dados\n",
    "df_train1.shape, df_val1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veja acima que temos 22 colunas (11 de cada tipo de modelo que criamos anteriormente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>./preds_train1\\lgbm_119_844_0.007210171877207321.pkl.z</th>\n",
       "      <th>./preds_train1\\lgbm_130_369_0.009060562467991657.pkl.z</th>\n",
       "      <th>./preds_train1\\lgbm_145_582_0.003446459897310035.pkl.z</th>\n",
       "      <th>./preds_train1\\lgbm_152_107_0.0029757772507264775.pkl.z</th>\n",
       "      <th>./preds_train1\\lgbm_161_521_0.004773968291550972.pkl.z</th>\n",
       "      <th>./preds_train1\\lgbm_170_624_0.002423157857285798.pkl.z</th>\n",
       "      <th>./preds_train1\\lgbm_24_910_0.01.pkl.z</th>\n",
       "      <th>./preds_train1\\lgbm_30_870_0.0029758295038874645.pkl.z</th>\n",
       "      <th>./preds_train1\\lgbm_61_58_0.0018735112038624302.pkl.z</th>\n",
       "      <th>./preds_train1\\lgbm_80_836_0.002174684006048089.pkl.z</th>\n",
       "      <th>...</th>\n",
       "      <th>./preds_train1\\mlp_123_170_0.0005198657849887135.pkl.z</th>\n",
       "      <th>./preds_train1\\mlp_133_80_0.0008209379223637906.pkl.z</th>\n",
       "      <th>./preds_train1\\mlp_147_121_0.00011878085823766309.pkl.z</th>\n",
       "      <th>./preds_train1\\mlp_154_30_8.855250245941234e-05.pkl.z</th>\n",
       "      <th>./preds_train1\\mlp_162_109_0.00022790773248734106.pkl.z</th>\n",
       "      <th>./preds_train1\\mlp_171_128_5.871694001325899e-05.pkl.z</th>\n",
       "      <th>./preds_train1\\mlp_200_60_0.001.pkl.z</th>\n",
       "      <th>./preds_train1\\mlp_37_175_8.855561236207113e-05.pkl.z</th>\n",
       "      <th>./preds_train1\\mlp_67_21_3.5100442309980525e-05.pkl.z</th>\n",
       "      <th>./preds_train1\\mlp_85_169_4.729250526161365e-05.pkl.z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.499353</td>\n",
       "      <td>0.491878</td>\n",
       "      <td>0.497622</td>\n",
       "      <td>0.494391</td>\n",
       "      <td>0.492374</td>\n",
       "      <td>0.496877</td>\n",
       "      <td>0.493495</td>\n",
       "      <td>0.498615</td>\n",
       "      <td>0.499352</td>\n",
       "      <td>0.499310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563139</td>\n",
       "      <td>0.499567</td>\n",
       "      <td>0.500720</td>\n",
       "      <td>0.484515</td>\n",
       "      <td>0.564859</td>\n",
       "      <td>0.470780</td>\n",
       "      <td>0.513688</td>\n",
       "      <td>0.501155</td>\n",
       "      <td>0.472986</td>\n",
       "      <td>0.442940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.511804</td>\n",
       "      <td>0.511407</td>\n",
       "      <td>0.504184</td>\n",
       "      <td>0.507694</td>\n",
       "      <td>0.506581</td>\n",
       "      <td>0.503241</td>\n",
       "      <td>0.506191</td>\n",
       "      <td>0.503027</td>\n",
       "      <td>0.501356</td>\n",
       "      <td>0.502488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441709</td>\n",
       "      <td>0.485821</td>\n",
       "      <td>0.393312</td>\n",
       "      <td>0.473000</td>\n",
       "      <td>0.473270</td>\n",
       "      <td>0.351067</td>\n",
       "      <td>0.474471</td>\n",
       "      <td>0.428056</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>0.356286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ./preds_train1\\lgbm_119_844_0.007210171877207321.pkl.z  \\\n",
       "0                                           0.499353        \n",
       "1                                           0.511804        \n",
       "\n",
       "   ./preds_train1\\lgbm_130_369_0.009060562467991657.pkl.z  \\\n",
       "0                                           0.491878        \n",
       "1                                           0.511407        \n",
       "\n",
       "   ./preds_train1\\lgbm_145_582_0.003446459897310035.pkl.z  \\\n",
       "0                                           0.497622        \n",
       "1                                           0.504184        \n",
       "\n",
       "   ./preds_train1\\lgbm_152_107_0.0029757772507264775.pkl.z  \\\n",
       "0                                           0.494391         \n",
       "1                                           0.507694         \n",
       "\n",
       "   ./preds_train1\\lgbm_161_521_0.004773968291550972.pkl.z  \\\n",
       "0                                           0.492374        \n",
       "1                                           0.506581        \n",
       "\n",
       "   ./preds_train1\\lgbm_170_624_0.002423157857285798.pkl.z  \\\n",
       "0                                           0.496877        \n",
       "1                                           0.503241        \n",
       "\n",
       "   ./preds_train1\\lgbm_24_910_0.01.pkl.z  \\\n",
       "0                               0.493495   \n",
       "1                               0.506191   \n",
       "\n",
       "   ./preds_train1\\lgbm_30_870_0.0029758295038874645.pkl.z  \\\n",
       "0                                           0.498615        \n",
       "1                                           0.503027        \n",
       "\n",
       "   ./preds_train1\\lgbm_61_58_0.0018735112038624302.pkl.z  \\\n",
       "0                                           0.499352       \n",
       "1                                           0.501356       \n",
       "\n",
       "   ./preds_train1\\lgbm_80_836_0.002174684006048089.pkl.z  ...  \\\n",
       "0                                           0.499310      ...   \n",
       "1                                           0.502488      ...   \n",
       "\n",
       "   ./preds_train1\\mlp_123_170_0.0005198657849887135.pkl.z  \\\n",
       "0                                           0.563139        \n",
       "1                                           0.441709        \n",
       "\n",
       "   ./preds_train1\\mlp_133_80_0.0008209379223637906.pkl.z  \\\n",
       "0                                           0.499567       \n",
       "1                                           0.485821       \n",
       "\n",
       "   ./preds_train1\\mlp_147_121_0.00011878085823766309.pkl.z  \\\n",
       "0                                           0.500720         \n",
       "1                                           0.393312         \n",
       "\n",
       "   ./preds_train1\\mlp_154_30_8.855250245941234e-05.pkl.z  \\\n",
       "0                                           0.484515       \n",
       "1                                           0.473000       \n",
       "\n",
       "   ./preds_train1\\mlp_162_109_0.00022790773248734106.pkl.z  \\\n",
       "0                                           0.564859         \n",
       "1                                           0.473270         \n",
       "\n",
       "   ./preds_train1\\mlp_171_128_5.871694001325899e-05.pkl.z  \\\n",
       "0                                           0.470780        \n",
       "1                                           0.351067        \n",
       "\n",
       "   ./preds_train1\\mlp_200_60_0.001.pkl.z  \\\n",
       "0                               0.513688   \n",
       "1                               0.474471   \n",
       "\n",
       "   ./preds_train1\\mlp_37_175_8.855561236207113e-05.pkl.z  \\\n",
       "0                                           0.501155       \n",
       "1                                           0.428056       \n",
       "\n",
       "   ./preds_train1\\mlp_67_21_3.5100442309980525e-05.pkl.z  \\\n",
       "0                                           0.472986       \n",
       "1                                           0.427700       \n",
       "\n",
       "   ./preds_train1\\mlp_85_169_4.729250526161365e-05.pkl.z  \n",
       "0                                           0.442940      \n",
       "1                                           0.356286      \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>./preds_val1\\lgbm_119_844_0.007210171877207321.pkl.z</th>\n",
       "      <th>./preds_val1\\lgbm_130_369_0.009060562467991657.pkl.z</th>\n",
       "      <th>./preds_val1\\lgbm_145_582_0.003446459897310035.pkl.z</th>\n",
       "      <th>./preds_val1\\lgbm_152_107_0.0029757772507264775.pkl.z</th>\n",
       "      <th>./preds_val1\\lgbm_161_521_0.004773968291550972.pkl.z</th>\n",
       "      <th>./preds_val1\\lgbm_170_624_0.002423157857285798.pkl.z</th>\n",
       "      <th>./preds_val1\\lgbm_24_910_0.01.pkl.z</th>\n",
       "      <th>./preds_val1\\lgbm_30_870_0.0029758295038874645.pkl.z</th>\n",
       "      <th>./preds_val1\\lgbm_61_58_0.0018735112038624302.pkl.z</th>\n",
       "      <th>./preds_val1\\lgbm_80_836_0.002174684006048089.pkl.z</th>\n",
       "      <th>...</th>\n",
       "      <th>./preds_val1\\mlp_123_170_0.0005198657849887135.pkl.z</th>\n",
       "      <th>./preds_val1\\mlp_133_80_0.0008209379223637906.pkl.z</th>\n",
       "      <th>./preds_val1\\mlp_147_121_0.00011878085823766309.pkl.z</th>\n",
       "      <th>./preds_val1\\mlp_154_30_8.855250245941234e-05.pkl.z</th>\n",
       "      <th>./preds_val1\\mlp_162_109_0.00022790773248734106.pkl.z</th>\n",
       "      <th>./preds_val1\\mlp_171_128_5.871694001325899e-05.pkl.z</th>\n",
       "      <th>./preds_val1\\mlp_200_60_0.001.pkl.z</th>\n",
       "      <th>./preds_val1\\mlp_37_175_8.855561236207113e-05.pkl.z</th>\n",
       "      <th>./preds_val1\\mlp_67_21_3.5100442309980525e-05.pkl.z</th>\n",
       "      <th>./preds_val1\\mlp_85_169_4.729250526161365e-05.pkl.z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.506658</td>\n",
       "      <td>0.503207</td>\n",
       "      <td>0.502306</td>\n",
       "      <td>0.501228</td>\n",
       "      <td>0.504455</td>\n",
       "      <td>0.501400</td>\n",
       "      <td>0.504083</td>\n",
       "      <td>0.500676</td>\n",
       "      <td>0.500038</td>\n",
       "      <td>0.500533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496100</td>\n",
       "      <td>0.490916</td>\n",
       "      <td>0.479123</td>\n",
       "      <td>0.493798</td>\n",
       "      <td>0.478058</td>\n",
       "      <td>0.498803</td>\n",
       "      <td>0.500976</td>\n",
       "      <td>0.510458</td>\n",
       "      <td>0.449284</td>\n",
       "      <td>0.521992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.502406</td>\n",
       "      <td>0.510168</td>\n",
       "      <td>0.505498</td>\n",
       "      <td>0.497800</td>\n",
       "      <td>0.506198</td>\n",
       "      <td>0.505437</td>\n",
       "      <td>0.504386</td>\n",
       "      <td>0.502276</td>\n",
       "      <td>0.500866</td>\n",
       "      <td>0.502103</td>\n",
       "      <td>...</td>\n",
       "      <td>0.572354</td>\n",
       "      <td>0.528258</td>\n",
       "      <td>0.495314</td>\n",
       "      <td>0.562607</td>\n",
       "      <td>0.542960</td>\n",
       "      <td>0.494304</td>\n",
       "      <td>0.565421</td>\n",
       "      <td>0.535983</td>\n",
       "      <td>0.452737</td>\n",
       "      <td>0.463671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ./preds_val1\\lgbm_119_844_0.007210171877207321.pkl.z  \\\n",
       "0                                           0.506658      \n",
       "1                                           0.502406      \n",
       "\n",
       "   ./preds_val1\\lgbm_130_369_0.009060562467991657.pkl.z  \\\n",
       "0                                           0.503207      \n",
       "1                                           0.510168      \n",
       "\n",
       "   ./preds_val1\\lgbm_145_582_0.003446459897310035.pkl.z  \\\n",
       "0                                           0.502306      \n",
       "1                                           0.505498      \n",
       "\n",
       "   ./preds_val1\\lgbm_152_107_0.0029757772507264775.pkl.z  \\\n",
       "0                                           0.501228       \n",
       "1                                           0.497800       \n",
       "\n",
       "   ./preds_val1\\lgbm_161_521_0.004773968291550972.pkl.z  \\\n",
       "0                                           0.504455      \n",
       "1                                           0.506198      \n",
       "\n",
       "   ./preds_val1\\lgbm_170_624_0.002423157857285798.pkl.z  \\\n",
       "0                                           0.501400      \n",
       "1                                           0.505437      \n",
       "\n",
       "   ./preds_val1\\lgbm_24_910_0.01.pkl.z  \\\n",
       "0                             0.504083   \n",
       "1                             0.504386   \n",
       "\n",
       "   ./preds_val1\\lgbm_30_870_0.0029758295038874645.pkl.z  \\\n",
       "0                                           0.500676      \n",
       "1                                           0.502276      \n",
       "\n",
       "   ./preds_val1\\lgbm_61_58_0.0018735112038624302.pkl.z  \\\n",
       "0                                           0.500038     \n",
       "1                                           0.500866     \n",
       "\n",
       "   ./preds_val1\\lgbm_80_836_0.002174684006048089.pkl.z  ...  \\\n",
       "0                                           0.500533    ...   \n",
       "1                                           0.502103    ...   \n",
       "\n",
       "   ./preds_val1\\mlp_123_170_0.0005198657849887135.pkl.z  \\\n",
       "0                                           0.496100      \n",
       "1                                           0.572354      \n",
       "\n",
       "   ./preds_val1\\mlp_133_80_0.0008209379223637906.pkl.z  \\\n",
       "0                                           0.490916     \n",
       "1                                           0.528258     \n",
       "\n",
       "   ./preds_val1\\mlp_147_121_0.00011878085823766309.pkl.z  \\\n",
       "0                                           0.479123       \n",
       "1                                           0.495314       \n",
       "\n",
       "   ./preds_val1\\mlp_154_30_8.855250245941234e-05.pkl.z  \\\n",
       "0                                           0.493798     \n",
       "1                                           0.562607     \n",
       "\n",
       "   ./preds_val1\\mlp_162_109_0.00022790773248734106.pkl.z  \\\n",
       "0                                           0.478058       \n",
       "1                                           0.542960       \n",
       "\n",
       "   ./preds_val1\\mlp_171_128_5.871694001325899e-05.pkl.z  \\\n",
       "0                                           0.498803      \n",
       "1                                           0.494304      \n",
       "\n",
       "   ./preds_val1\\mlp_200_60_0.001.pkl.z  \\\n",
       "0                             0.500976   \n",
       "1                             0.565421   \n",
       "\n",
       "   ./preds_val1\\mlp_37_175_8.855561236207113e-05.pkl.z  \\\n",
       "0                                           0.510458     \n",
       "1                                           0.535983     \n",
       "\n",
       "   ./preds_val1\\mlp_67_21_3.5100442309980525e-05.pkl.z  \\\n",
       "0                                           0.449284     \n",
       "1                                           0.452737     \n",
       "\n",
       "   ./preds_val1\\mlp_85_169_4.729250526161365e-05.pkl.z  \n",
       "0                                           0.521992    \n",
       "1                                           0.463671    \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val1.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking: Nível 1 - Criação do Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegamos o dataset com as 22 previsões (df_train1) e o y do segundo bloco de dados (y_train1). Vamos testar 3 tipos de modelos diferentes para o nível 1 do nosso Stacking.\n",
    "\n",
    "spearmanr -> Maior é melhor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03381981023149141"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "#mdl = LinearRegression(normalize=True)\n",
    "mdl = Ridge(alpha=1.)\n",
    "\n",
    "mdl.fit(df_train1, y_train1)\n",
    "\n",
    "p = mdl.predict(df_val1)\n",
    "\n",
    "# maior é melhor\n",
    "spearmanr(y_val1, p).correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02558809717435016"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "\n",
    "mdl = ExtraTreesRegressor(n_estimators=100, \n",
    "                          min_samples_leaf=100,\n",
    "                          random_state=0, \n",
    "                          n_jobs=-1)\n",
    "\n",
    "mdl.fit(df_train1, y_train1)\n",
    "\n",
    "p = mdl.predict(df_val1)\n",
    "\n",
    "# maior é melhor\n",
    "spearmanr(y_val1, p).correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: min_samples_leaf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.032734449882776004"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "mdl = LGBMRegressor(n_estimators=200,\n",
    "                    min_samples_leaf=10, \n",
    "                    num_leaves=5, \n",
    "                    colsample_bytree=0.1,\n",
    "                    random_state=0, \n",
    "                    n_jobs=-1,\n",
    "                    learning_rate=1e-2)\n",
    "\n",
    "mdl.fit(df_train1, y_train1)\n",
    "\n",
    "p = mdl.predict(df_val1)\n",
    "\n",
    "# maior é melhor\n",
    "spearmanr(y_val1, p).correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melhores scores nos modelos individuais (maior é melhor)\n",
    "\n",
    "for p_name, score in scores_val1.items():\n",
    "    \n",
    "    if score > 0.035:\n",
    "        print(p_name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pegamos o melhor modelo acima (Ridge) e vamos aplicar aos dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = glob.glob(\"./preds_test/*.pkl.z\")\n",
    "\n",
    "df_test = [] \n",
    "scores_test = dict()\n",
    "\n",
    "for p_name in preds_test:\n",
    "    \n",
    "    p = jb.load(p_name)\n",
    "    p_df = pd.DataFrame(p, columns=[p_name])\n",
    "    df_test.append(p_df)\n",
    "    scores_test[p_name] = spearmanr(y_test, p_df).correlation\n",
    "    \n",
    "\n",
    "df_test = pd.concat(df_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013858976231144434"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "#mdl = LinearRegression(normalize=True)\n",
    "mdl = Ridge(alpha=1.)\n",
    "\n",
    "mdl.fit(df_train1, y_train1)\n",
    "\n",
    "p = mdl.predict(df_test)\n",
    "\n",
    "spearmanr(y_test, p).correlation\n",
    "#test spearman = 0.0212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melhores scores nos modelos individuais (maior é melhor)\n",
    "\n",
    "for p_name, score in scores_test.items():\n",
    "    \n",
    "    if score > 0.02:\n",
    "        print(p_name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando o Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisamos a pontual dos modelos em dados de validação (bloco 3) e os dados de teste. Buscamos algo mais próximo de uma regressão perfeita para evidenciar que o modelo aplicado a dados de validação se sai bem também em dados de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_val1_series = pd.Series(scores_val1)\n",
    "scores_test_series = pd.Series(scores_test)\n",
    "\n",
    "import seaborn as sns\n",
    "%pylab inline\n",
    "\n",
    "fig, ax = pylab.subplots(1,1, figsize=(15,10))\n",
    "\n",
    "sns.regplot(x=scores_val1_series, y=scores_test_series, ax=ax)\n",
    "\n",
    "ax.plot(ax.get_xlim(), ax.get_ylim(), ls=\"--\", c=\".3\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando Diversidade: Combinações de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_groups = ['feature_intelligence', \n",
    "                  'feature_charisma', \n",
    "                  'feature_strength', \n",
    "                  'feature_dexterity', \n",
    "                  'feature_constitution', \n",
    "                  'feature_wisdom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM para 1 Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treina um modelo em cada feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence 0.00374063608434927\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_charisma 0.01113070841236179\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_strength 0.006697064024431434\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_dexterity 0.009411367200089676\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_constitution 0.0060611470084215215\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_wisdom 0.004598225504345016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# loop\n",
    "for group in feature_groups:\n",
    "    \n",
    "    # Pegamos os melhores parâmetros que encontramos naquele treinamento realizado anteriormente\n",
    "    num_leaves, min_data_in_leaf, learning_rate = [130, 369, 0.009]\n",
    "    \n",
    "    \n",
    "    mdl = LGBMRegressor(num_leaves=num_leaves,\n",
    "                        min_data_in_leaf=min_data_in_leaf,\n",
    "                        learning_rate=learning_rate, \n",
    "                        n_estimators=100, \n",
    "                        random_state=0)\n",
    "    \n",
    "    mdl.fit(X_train0.filter(regex=group, axis=1), y_train0)\n",
    "\n",
    "    p = mdl.predict(X_train1.filter(regex=group,axis=1))\n",
    "    \n",
    "    model_name_train1 = \"./preds_train1/lgbm_feature_groups_{}.pkl.z\".format(group) \n",
    "    jb.dump(p, model_name_train1)\n",
    "\n",
    "    metric = spearmanr(y_train1.values,p).correlation\n",
    "\n",
    "    p = mdl.predict(X_val1.filter(regex=group,axis=1))\n",
    "    \n",
    "    model_name_val1 = \"./preds_val1/lgbm_feature_groups_{}.pkl.z\".format(group) \n",
    "    jb.dump(p, model_name_val1)\n",
    "\n",
    "    p = mdl.predict(X_test.filter(regex=group,axis=1))\n",
    "    \n",
    "    model_name_test = \"./preds_test/lgbm_feature_groups_{}.pkl.z\".format(group) \n",
    "    jb.dump(p, model_name_test)\n",
    "\n",
    "    print(group, metric)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM para 2 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora treinamos um modelo para cada combinação de duas features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_charisma 0.018111292644637546\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_strength 0.006629157247998499\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_dexterity 0.0105771090584426\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_constitution 0.012879506778302852\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_wisdom 0.010342135317408909\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_charisma feature_strength 0.008848863213145177\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_charisma feature_dexterity 0.021427559265427592\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_charisma feature_constitution 0.013227213458345476\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_charisma feature_wisdom 0.014267935246420454\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_strength feature_dexterity 0.01532778901720834\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_strength feature_constitution 0.007970544536115053\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_strength feature_wisdom 0.008474083770806348\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_dexterity feature_constitution 0.015001776146955003\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_dexterity feature_wisdom 0.02005374809661212\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_constitution feature_wisdom 0.01397944322809205\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for group1, group2 in combinations(feature_groups, 2):\n",
    "    \n",
    "    num_leaves, min_data_in_leaf, learning_rate = [130, 369, 0.009]\n",
    "    \n",
    "    mdl = LGBMRegressor(num_leaves=num_leaves,\n",
    "                        min_data_in_leaf=min_data_in_leaf,\n",
    "                        learning_rate=learning_rate, \n",
    "                        n_estimators=100,\n",
    "                        random_state=0)\n",
    "    \n",
    "    selected = []\n",
    "    \n",
    "    for f in X_train0.columns:\n",
    "        if group1 in f or group2 in f:\n",
    "            selected.append(f)\n",
    "\n",
    "    mdl.fit(X_train0[selected], y_train0)\n",
    "\n",
    "    p = mdl.predict(X_train1[selected])\n",
    "    \n",
    "    model_name_train1 = \"./preds_train1/lgbm_feature_groups_{}_{}.pkl.z\".format(group1, group2) \n",
    "    jb.dump(p, model_name_train1)\n",
    "\n",
    "    metric = spearmanr(y_train1.values,p).correlation\n",
    "\n",
    "    p = mdl.predict(X_val1[selected])\n",
    "    \n",
    "    model_name_val1 = \"./preds_val1/lgbm_feature_groups_{}_{}.pkl.z\".format(group1, group2) \n",
    "    jb.dump(p, model_name_val1)\n",
    "\n",
    "    p = mdl.predict(X_test[selected])\n",
    "    \n",
    "    model_name_test = \"./preds_test/lgbm_feature_groups_{}_{}.pkl.z\".format(group1, group2) \n",
    "    jb.dump(p, model_name_test)\n",
    "\n",
    "    print(group1, group2, metric)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM para 3 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora treinamos um modelo para cada combinação de três features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_charisma feature_strength 0.01341732390308576\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_charisma feature_dexterity 0.022151914400485296\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_charisma feature_constitution 0.018827256872208417\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_charisma feature_wisdom 0.017326460768674265\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_strength feature_dexterity 0.019416040984825442\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_strength feature_constitution 0.009866302139442404\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_strength feature_wisdom 0.009751084806796772\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_dexterity feature_constitution 0.018825995398211164\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_dexterity feature_wisdom 0.022527159106019642\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_intelligence feature_constitution feature_wisdom 0.016116884957243317\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_charisma feature_strength feature_dexterity 0.02011484312024728\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_charisma feature_strength feature_constitution 0.010078936168523267\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_charisma feature_strength feature_wisdom 0.01253929458377776\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_charisma feature_dexterity feature_constitution 0.022965897822794935\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_charisma feature_dexterity feature_wisdom 0.024166060515670485\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_charisma feature_constitution feature_wisdom 0.01535842611720392\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_strength feature_dexterity feature_constitution 0.017370785614840366\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_strength feature_dexterity feature_wisdom 0.01895695529841967\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_strength feature_constitution feature_wisdom 0.008716102364079513\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=369, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=369\n",
      "feature_dexterity feature_constitution feature_wisdom 0.016197050934017156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for group1, group2, group3 in combinations(feature_groups, 3):\n",
    "    \n",
    "    num_leaves, min_data_in_leaf, learning_rate = [130, 369, 0.009]\n",
    "    \n",
    "    \n",
    "    mdl = LGBMRegressor(num_leaves=num_leaves,\n",
    "                        min_data_in_leaf=min_data_in_leaf,\n",
    "                        learning_rate=learning_rate, \n",
    "                        n_estimators=100,\n",
    "                        random_state=0)\n",
    "    \n",
    "    selected = []\n",
    "    \n",
    "    for f in X_train0.columns:\n",
    "        if group1 in f or group2 in f or group3 in f:\n",
    "            selected.append(f)\n",
    "\n",
    "    \n",
    "    mdl.fit(X_train0[selected], y_train0)\n",
    "\n",
    "    p = mdl.predict(X_train1[selected])\n",
    "    \n",
    "    model_name_train1 = \"./preds_train1/lgbm_feature_groups_{}_{}_{}.pkl.z\".format(group1, group2, group3) \n",
    "    jb.dump(p, model_name_train1)\n",
    "\n",
    "    metric = spearmanr(y_train1.values,p).correlation\n",
    "\n",
    "    p = mdl.predict(X_val1[selected])\n",
    "    \n",
    "    model_name_val1 = \"./preds_val1/lgbm_feature_groups_{}_{}_{}.pkl.z\".format(group1, group2, group3) \n",
    "    jb.dump(p, model_name_val1)\n",
    "\n",
    "    p = mdl.predict(X_test[selected])\n",
    "    \n",
    "    model_name_test = \"./preds_test/lgbm_feature_groups_{}_{}_{}.pkl.z\".format(group1, group2, group3) \n",
    "    jb.dump(p, model_name_test)\n",
    "\n",
    "    print(group1, group2, group3, metric)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando Diversidade: Modelos Diferentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede Neural com Arquitetura Diferente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressorTorch(nn.Module):\n",
    "    def __init__(self, input_size, hidden1, hidden2, hidden3, drop1, drop2, drop3):\n",
    "        super(MLPRegressorTorch,self).__init__()\n",
    "        self.hidden1 = nn.Linear(input_size, hidden1)\n",
    "        self.drop1 = nn.Dropout(drop1)\n",
    "        self.hidden2 = nn.Linear(hidden1, hidden2)\n",
    "        self.drop2 = nn.Dropout(drop2)\n",
    "        self.hidden3 = nn.Linear(hidden2, hidden3)\n",
    "        self.drop3 = nn.Dropout(drop3)\n",
    "        self.out = nn.Linear(hidden3, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.drop2(x)\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = self.drop3(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "    \n",
    "#MLPRegressorTorch(X_train0.shape[1], 10, 10)(torch.from_numpy(X_train0.values).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "\n",
    "X_train0_t = torch.from_numpy(X_train0.values).float().cuda()\n",
    "y_train0_t = torch.from_numpy(y_train0.values).float().unsqueeze(dim=-1).cuda()\n",
    "\n",
    "X_train1_t = torch.from_numpy(X_train1.values).float().cuda()\n",
    "X_val1_t = torch.from_numpy(X_val1.values).float().cuda()\n",
    "X_test_t = torch.from_numpy(X_test.values).float().cuda()\n",
    "\n",
    "\n",
    "def tune_mlp(params):\n",
    "    #scaling?\n",
    "    torch.manual_seed(0)\n",
    "    hidden1, hidden2, hidden3, drop1, drop2, drop3, learning_rate = params\n",
    "    mdl = MLPRegressorTorch(X_train0.shape[1], hidden1, hidden2, hidden3, drop1, drop2, drop3).cuda()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(mdl.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(300): \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        p = mdl(X_train0_t)\n",
    "        loss = criterion(p, y_train0_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    p = mdl(X_train1_t).detach().cpu().numpy()\n",
    "    model_name_train1 = \"./preds_train1/mlp_{}_{}_{}_{}_{}_{}_{}.pkl.z\".format(hidden1, \n",
    "                                                                   hidden2, \n",
    "                                                                   hidden3,\n",
    "                                                                   drop1,\n",
    "                                                                   drop2,\n",
    "                                                                   drop3,\n",
    "                                                                   learning_rate) \n",
    "    jb.dump(p, model_name_train1)\n",
    "    #print(p)\n",
    "    \n",
    "    metric = spearmanr(y_train1.values,p).correlation\n",
    "    \n",
    "    p = mdl(X_val1_t).detach().cpu().numpy()\n",
    "    model_name_val1 = \"./preds_val1/mlp_{}_{}_{}_{}_{}_{}_{}.pkl.z\".format(hidden1, \n",
    "                                                                   hidden2, \n",
    "                                                                   hidden3,\n",
    "                                                                   drop1,\n",
    "                                                                   drop2,\n",
    "                                                                   drop3,\n",
    "                                                                   learning_rate)\n",
    "    jb.dump(p, model_name_val1)\n",
    "    \n",
    "    p = mdl(X_test_t).detach().cpu().numpy()\n",
    "    model_name_test = \"./preds_test/mlp_{}_{}_{}_{}_{}_{}_{}.pkl.z\".format(hidden1, \n",
    "                                                                   hidden2, \n",
    "                                                                   hidden3,\n",
    "                                                                   drop1,\n",
    "                                                                   drop2,\n",
    "                                                                   drop3,\n",
    "                                                                   learning_rate) \n",
    "    jb.dump(p, model_name_test)\n",
    "    \n",
    "    print(params, metric)\n",
    "    print()\n",
    "    \n",
    "    return -metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 754.00 MiB (GPU 0; 4.00 GiB total capacity; 2.26 GiB already allocated; 739.39 MiB free; 2.26 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-55eaea180350>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m          (1e-6, 1e-3, 'log-uniform')]\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgp_minimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtune_mlp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs, model_queue_size)\u001b[0m\n\u001b[0;32m    257\u001b[0m             noise=noise)\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m     return base_minimize(\n\u001b[0m\u001b[0;32m    260\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[0macq_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0macq_func\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\skopt\\optimizer\\base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[1;34m(func, dimensions, base_estimator, n_calls, n_random_starts, n_initial_points, initial_point_generator, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs, model_queue_size)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_calls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[0mnext_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[0mnext_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-a259b95f04ba>\u001b[0m in \u001b[0;36mtune_mlp\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train0_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train0_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-57-9e9f3c870b75>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1688\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1690\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 754.00 MiB (GPU 0; 4.00 GiB total capacity; 2.26 GiB already allocated; 739.39 MiB free; 2.26 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "space = [(500, 2000),\n",
    "         (500, 2000),\n",
    "         (500, 2000),\n",
    "         (0.0, 0.95),\n",
    "         (0.0, 0.95),\n",
    "         (0.0, 0.95),\n",
    "         (1e-6, 1e-3, 'log-uniform')]\n",
    "\n",
    "res = gp_minimize(tune_mlp, space, random_state=0, verbose=1, n_calls=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB com Todas as Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi melhor que a nossa stacking inicial toda... mas note que usamos 2000 árvores, ao contrario das 100 que tinhamos utilizado antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03794601484541533\n"
     ]
    }
   ],
   "source": [
    "mdl = XGBRegressor(max_depth=5, \n",
    "                   learning_rate=0.01, \n",
    "                   n_estimators=2000, \n",
    "                   colsample_bytree=0.1, \n",
    "                   #tree_method='gpu_hist', \n",
    "                   gpu_id=0)\n",
    "\n",
    "mdl.fit(X_train0, y_train0)\n",
    "\n",
    "p = mdl.predict(X_train1)\n",
    "\n",
    "model_name_train1 = \"./preds_train1/xgb_default.pkl.z\"\n",
    "jb.dump(p, model_name_train1)\n",
    "\n",
    "metric = spearmanr(y_train1.values,p).correlation\n",
    "\n",
    "p = mdl.predict(X_val1)\n",
    "\n",
    "model_name_val1 = \"./preds_val1/xgb_default.pkl.z\" \n",
    "jb.dump(p, model_name_val1)\n",
    "\n",
    "p = mdl.predict(X_test)\n",
    "\n",
    "model_name_test = \"./preds_test/xgb_default.pkl.z\"\n",
    "jb.dump(p, model_name_test)\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB para 1 Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_intelligence 0.004579789785842799\n",
      "\n",
      "feature_charisma 0.011202050022626642\n",
      "\n",
      "feature_strength 0.009468585723825215\n",
      "\n",
      "feature_dexterity 0.010706552360754863\n",
      "\n",
      "feature_constitution 0.00946472199603127\n",
      "\n",
      "feature_wisdom 0.011403077777362953\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_groups = ['feature_intelligence', \n",
    "                  'feature_charisma', \n",
    "                  'feature_strength', \n",
    "                  'feature_dexterity', \n",
    "                  'feature_constitution', \n",
    "                  'feature_wisdom']\n",
    "\n",
    "for group in feature_groups:\n",
    "    mdl = XGBRegressor(max_depth=5, \n",
    "                       learning_rate=0.01, \n",
    "                       n_estimators=2000, \n",
    "                       colsample_bytree=0.1, \n",
    "                       #tree_method='gpu_hist', \n",
    "                       gpu_id=0)\n",
    "    \n",
    "    mdl.fit(X_train0.filter(regex=group,axis=1), y_train0)\n",
    "\n",
    "    p = mdl.predict(X_train1.filter(regex=group,axis=1))\n",
    "    \n",
    "    model_name_train1 = \"./preds_train1/xgb_feature_groups_{}.pkl.z\".format(group) \n",
    "    jb.dump(p, model_name_train1)\n",
    "\n",
    "    metric = spearmanr(y_train1.values,p).correlation\n",
    "\n",
    "    p = mdl.predict(X_val1.filter(regex=group,axis=1))\n",
    "    \n",
    "    model_name_val1 = \"./preds_val1/xgb_feature_groups_{}.pkl.z\".format(group) \n",
    "    jb.dump(p, model_name_val1)\n",
    "\n",
    "    p = mdl.predict(X_test.filter(regex=group,axis=1))\n",
    "    \n",
    "    model_name_test = \"./preds_test/xgb_feature_groups_{}.pkl.z\".format(group) \n",
    "    jb.dump(p, model_name_test)\n",
    "\n",
    "    print(group, metric)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB para 2 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_intelligence feature_charisma 0.019793489077801728\n",
      "\n",
      "feature_intelligence feature_strength 0.014698624866872938\n",
      "\n",
      "feature_intelligence feature_dexterity 0.016303557454322527\n",
      "\n",
      "feature_intelligence feature_constitution 0.01667758694274272\n",
      "\n",
      "feature_intelligence feature_wisdom 0.01651476234315524\n",
      "\n",
      "feature_charisma feature_strength 0.01609290721375223\n",
      "\n",
      "feature_charisma feature_dexterity 0.022248943663177135\n",
      "\n",
      "feature_charisma feature_constitution 0.017361863745583354\n",
      "\n",
      "feature_charisma feature_wisdom 0.020585756102606535\n",
      "\n",
      "feature_strength feature_dexterity 0.019246912724846122\n",
      "\n",
      "feature_strength feature_constitution 0.011981485399881192\n",
      "\n",
      "feature_strength feature_wisdom 0.013711108763284465\n",
      "\n",
      "feature_dexterity feature_constitution 0.02259955221315074\n",
      "\n",
      "feature_dexterity feature_wisdom 0.02299917641865188\n",
      "\n",
      "feature_constitution feature_wisdom 0.015698667240184726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for group1, group2 in combinations(feature_groups, 2):\n",
    "    \n",
    "    mdl = XGBRegressor(max_depth=5, \n",
    "                       learning_rate=0.01, \n",
    "                       n_estimators=2000, \n",
    "                       colsample_bytree=0.1, \n",
    "                       #tree_method='gpu_hist', \n",
    "                       gpu_id=0)\n",
    "  \n",
    "    \n",
    "    selected = []\n",
    "    \n",
    "    for f in X_train0.columns:\n",
    "        if group1 in f or group2 in f:\n",
    "            selected.append(f)\n",
    "\n",
    "    \n",
    "    mdl.fit(X_train0[selected], y_train0)\n",
    "\n",
    "    p = mdl.predict(X_train1[selected])\n",
    "    \n",
    "    model_name_train1 = \"./preds_train1/xgb_feature_groups_{}_{}.pkl.z\".format(group1, group2) \n",
    "    jb.dump(p, model_name_train1)\n",
    "\n",
    "    metric = spearmanr(y_train1.values,p).correlation\n",
    "\n",
    "    p = mdl.predict(X_val1[selected])\n",
    "    \n",
    "    model_name_val1 = \"./preds_val1/xgb_feature_groups_{}_{}.pkl.z\".format(group1, group2) \n",
    "    jb.dump(p, model_name_val1)\n",
    "\n",
    "    p = mdl.predict(X_test[selected])\n",
    "    \n",
    "    model_name_test = \"./preds_test/xgb_feature_groups_{}_{}.pkl.z\".format(group1, group2) \n",
    "    jb.dump(p, model_name_test)\n",
    "\n",
    "    print(group1, group2, metric)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGB para 3 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_intelligence feature_charisma feature_strength 0.021168226570951954\n",
      "\n",
      "feature_intelligence feature_charisma feature_dexterity 0.026853476152643625\n",
      "\n",
      "feature_intelligence feature_charisma feature_constitution 0.0238207944905206\n",
      "\n",
      "feature_intelligence feature_charisma feature_wisdom 0.025974794745050538\n",
      "\n",
      "feature_intelligence feature_strength feature_dexterity 0.023046342704775433\n",
      "\n",
      "feature_intelligence feature_strength feature_constitution 0.01889545540184942\n",
      "\n",
      "feature_intelligence feature_strength feature_wisdom 0.019436828251397375\n",
      "\n",
      "feature_intelligence feature_dexterity feature_constitution 0.027136582162398084\n",
      "\n",
      "feature_intelligence feature_dexterity feature_wisdom 0.025996225350558073\n",
      "\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[23:06:32] C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/tree/updater_gpu_hist.cu:786: Exception in gpu_hist: [23:06:32] c:\\users\\administrator\\workspace\\xgboost-win64_release_1.3.0\\src\\data\\../common/device_helpers.cuh:400: Memory allocation error on worker 0: [23:06:32] c:\\users\\administrator\\workspace\\xgboost-win64_release_1.3.0\\src\\common\\common.h:44: c:\\users\\administrator\\workspace\\xgboost-win64_release_1.3.0\\src\\common\\device_helpers.cuh: 419: cudaErrorMemoryAllocation: out of memory\n- Free memory: 193803060\n- Requested memory: 195525472\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-bced3b55d233>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mselected\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mmdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train0\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselected\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselected\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m    595\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'eval_metric'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         self._Booster = train(params, train_dmatrix,\n\u001b[0m\u001b[0;32m    598\u001b[0m                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \"\"\"\n\u001b[1;32m--> 227\u001b[1;33m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    228\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1280\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1281\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1282\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \"\"\"\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [23:06:32] C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/tree/updater_gpu_hist.cu:786: Exception in gpu_hist: [23:06:32] c:\\users\\administrator\\workspace\\xgboost-win64_release_1.3.0\\src\\data\\../common/device_helpers.cuh:400: Memory allocation error on worker 0: [23:06:32] c:\\users\\administrator\\workspace\\xgboost-win64_release_1.3.0\\src\\common\\common.h:44: c:\\users\\administrator\\workspace\\xgboost-win64_release_1.3.0\\src\\common\\device_helpers.cuh: 419: cudaErrorMemoryAllocation: out of memory\n- Free memory: 193803060\n- Requested memory: 195525472\n\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "for group1, group2, group3 in combinations(feature_groups, 3):\n",
    "    \n",
    "    mdl = XGBRegressor(max_depth=5, \n",
    "                       learning_rate=0.01, \n",
    "                       n_estimators=2000, \n",
    "                       colsample_bytree=0.1, \n",
    "                       tree_method='gpu_hist', \n",
    "                       gpu_id=0)\n",
    "      \n",
    "    selected = []\n",
    "    \n",
    "    for f in X_train0.columns:\n",
    "        if group1 in f or group2 in f or group3 in f:\n",
    "            selected.append(f)\n",
    "    \n",
    "    mdl.fit(X_train0[selected], y_train0)\n",
    "\n",
    "    p = mdl.predict(X_train1[selected])\n",
    "    \n",
    "    model_name_train1 = \"./preds_train1/xgb_feature_groups_{}_{}_{}.pkl.z\".format(group1, group2, group3) \n",
    "    jb.dump(p, model_name_train1)\n",
    "\n",
    "    metric = spearmanr(y_train1.values,p).correlation\n",
    "\n",
    "    p = mdl.predict(X_val1[selected])\n",
    "    \n",
    "    model_name_val1 = \"./preds_val1/xgb_feature_groups_{}_{}_{}.pkl.z\".format(group1, group2, group3) \n",
    "    jb.dump(p, model_name_val1)\n",
    "\n",
    "    p = mdl.predict(X_test[selected])\n",
    "    \n",
    "    model_name_test = \"./preds_test/xgb_feature_groups_{}_{}_{}.pkl.z\".format(group1, group2, group3) \n",
    "    jb.dump(p, model_name_test)\n",
    "\n",
    "    print(group1, group2, group3, metric)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando Diversidade: Combinações de Linhas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação Por Soma Valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na separação de linhas, se houver uma variável categorica, tentamos separar as linhas por essa variável (exemplo: Todos os registros de um estado vão ser uma separação/bloco de linhas).\n",
    "\n",
    "Em nosso caso não temos isso. Então fizemos uma separação por valor total da soma das linhas, separando-as pela soma total, dessa forma, linhas com valores semelhantes de soma ficam em um mesmo grupo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train0.sum(axis=1).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02759297403152932\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterar por colunas de grupos se tivesse mais categoricas\n",
    "buckets_Xtr0 = np.digitize(X_train0.sum(axis=1), bins=[125,150,175])\n",
    "buckets_Xtr1 = np.digitize(X_train1.sum(axis=1), bins=[125,150,175])\n",
    "buckets_Xvl1 = np.digitize(X_val1.sum(axis=1), bins=[125,150,175])\n",
    "buckets_Xts = np.digitize(X_test.sum(axis=1), bins=[125,150,175])\n",
    "\n",
    "\n",
    "p_tr1 = np.zeros(X_train1.shape[0])\n",
    "p_vl1 = np.zeros(X_val1.shape[0])\n",
    "p_ts = np.zeros(X_test.shape[0])\n",
    "\n",
    "for bucket in np.unique(buckets_Xtr0):\n",
    "    \n",
    "    Xtr0 = X_train0[buckets_Xtr0 == bucket]\n",
    "    ytr0 = y_train0[buckets_Xtr0 == bucket]\n",
    "    \n",
    "    mdl = XGBRegressor(max_depth=5, \n",
    "                       learning_rate=0.01, \n",
    "                       n_estimators=2000, \n",
    "                       colsample_bytree=0.1, \n",
    "                       #tree_method='gpu_hist', \n",
    "                       gpu_id=0)\n",
    "    \n",
    "    mdl.fit(Xtr0, ytr0)\n",
    "    \n",
    "    Xtr1 = X_train1[buckets_Xtr1 == bucket]\n",
    "    p_tr1[buckets_Xtr1 == bucket] = mdl.predict(Xtr1)\n",
    "    \n",
    "    Xvl1 = X_val1[buckets_Xvl1 == bucket]\n",
    "    p_vl1[buckets_Xvl1 == bucket] = mdl.predict(Xvl1)\n",
    "    \n",
    "    Xts = X_test[buckets_Xts == bucket]\n",
    "    p_ts[buckets_Xts == bucket] = mdl.predict(Xts)\n",
    "\n",
    "    \n",
    "model_name_train1 = \"./preds_train1/xgb_row_groups_buckets.pkl.z\"\n",
    "jb.dump(p_tr1, model_name_train1)\n",
    "\n",
    "metric = spearmanr(y_train1.values,p_tr1).correlation\n",
    "\n",
    "model_name_val1 = \"./preds_val1/xgb_row_groups_buckets.pkl.z\"\n",
    "jb.dump(p_vl1, model_name_val1)\n",
    "\n",
    "model_name_test = \"./preds_test/xgb_row_groups_buckets.pkl.z\"\n",
    "jb.dump(p_ts, model_name_test)\n",
    "\n",
    "print(metric)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação Por Correlação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semelhante ao anterior, mas agora buscamos as features com maior correlação (positiva ou negativa) com a target e pagamos as 10 mais correlacionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_constitution30 0.025949039636915756\n",
      "\n",
      "feature_charisma37 0.027526460821600384\n",
      "\n",
      "feature_charisma69 0.023645841348983038\n",
      "\n",
      "feature_charisma19 0.027577746902233618\n",
      "\n",
      "feature_wisdom42 0.029037658521371867\n",
      "\n",
      "feature_wisdom36 0.028352388252477906\n",
      "\n",
      "feature_strength34 0.021693348223641606\n",
      "\n",
      "feature_strength14 0.02040854674801827\n",
      "\n",
      "feature_dexterity7 0.025865277697509688\n",
      "\n",
      "feature_dexterity4 0.029745931426478488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = X_train0.corrwith(y_train0, method='spearman')\n",
    "most_corr = c.abs().sort_values().tail(10).index\n",
    "\n",
    "\n",
    "#iterar por colunas de grupos se tivesse mais categoricas\n",
    "for col in most_corr:\n",
    "\n",
    "    p_tr1 = np.zeros(X_train1.shape[0])\n",
    "    p_vl1 = np.zeros(X_val1.shape[0])\n",
    "    p_ts = np.zeros(X_test.shape[0])\n",
    "\n",
    "    for bucket in X_train0[col].unique():\n",
    "\n",
    "        Xtr0 = X_train0[X_train0[col] == bucket]\n",
    "        ytr0 = y_train0[X_train0[col] == bucket]\n",
    "\n",
    "        mdl = XGBRegressor(max_depth=5, \n",
    "                           learning_rate=0.01, \n",
    "                           n_estimators=2000, \n",
    "                           colsample_bytree=0.1, \n",
    "                           #tree_method='gpu_hist', \n",
    "                           gpu_id=0)\n",
    "        \n",
    "        mdl.fit(Xtr0, ytr0)\n",
    "\n",
    "        Xtr1 = X_train1[X_train1[col] == bucket]\n",
    "        p_tr1[X_train1[col] == bucket] = mdl.predict(Xtr1)\n",
    "\n",
    "        Xvl1 = X_val1[X_val1[col] == bucket]\n",
    "        p_vl1[X_val1[col] == bucket] = mdl.predict(Xvl1)\n",
    "\n",
    "        Xts = X_test[X_test[col] == bucket]\n",
    "        p_ts[X_test[col] == bucket] = mdl.predict(Xts)\n",
    "\n",
    "\n",
    "    model_name_train1 = \"./preds_train1/xgb_row_groups_most_corr_{}.pkl.z\".format(col)\n",
    "    jb.dump(p_tr1, model_name_train1)\n",
    "\n",
    "    metric = spearmanr(y_train1.values,p_tr1).correlation\n",
    "\n",
    "    model_name_val1 = \"./preds_val1/xgb_row_groups_most_corr_{}.pkl.z\".format(col)\n",
    "    jb.dump(p_vl1, model_name_val1)\n",
    "\n",
    "    model_name_test = \"./preds_test/xgb_row_groups_most_corr_{}.pkl.z\".format(col)\n",
    "    jb.dump(p_ts, model_name_test)\n",
    "\n",
    "    print(col, metric)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação Por Tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos as linhas com base nas eras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.02310342060393435\n",
      "\n",
      "1 0.026401032453119033\n",
      "\n",
      "2 0.026725069622356507\n",
      "\n",
      "3 0.027500300539836955\n",
      "\n",
      "4 0.010149563903710615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "buckets_Xtr0 = np.digitize(train_train_level0['era'], bins=[10, 20, 30, 40])\n",
    "\n",
    "for bucket in np.unique(buckets_Xtr0):\n",
    "\n",
    "    Xtr0 = X_train0[buckets_Xtr0 == bucket]\n",
    "    ytr0 = y_train0[buckets_Xtr0 == bucket]\n",
    "\n",
    "    mdl = XGBRegressor(max_depth=5, \n",
    "                       learning_rate=0.01, \n",
    "                       n_estimators=2000, \n",
    "                       colsample_bytree=0.1, \n",
    "                       #tree_method='gpu_hist', \n",
    "                       gpu_id=0)\n",
    "    \n",
    "    mdl.fit(Xtr0, ytr0)\n",
    "\n",
    "    p = mdl.predict(X_train1)\n",
    "    model_name_train1 = \"./preds_train1/xgb_row_groups_eras_{}.pkl.z\".format(bucket)\n",
    "    jb.dump(p, model_name_train1)\n",
    "\n",
    "    metric = spearmanr(y_train1.values,p).correlation\n",
    "\n",
    "    p = mdl.predict(X_val1)\n",
    "    model_name_val1 = \"./preds_val1/xgb_row_groups_eras_{}.pkl.z\".format(bucket)\n",
    "    jb.dump(p, model_name_val1)\n",
    "\n",
    "    p = mdl.predict(X_test)\n",
    "    model_name_test = \"./preds_test/xgb_row_groups_eras_{}.pkl.z\".format(bucket)\n",
    "    jb.dump(p, model_name_test)\n",
    "\n",
    "\n",
    "    print(bucket, metric)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando Diversidade: Hiperparâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023484027981592888\n",
      "0.03794601484541533\n",
      "0.032266006786126976\n"
     ]
    }
   ],
   "source": [
    "# Profundidade rasa, média e profunda (risco overfitting)\n",
    "for depth in [1, 5, 10]:\n",
    "    \n",
    "    mdl = XGBRegressor(max_depth=depth,\n",
    "                       learning_rate=0.01,\n",
    "                       n_estimators=2000,\n",
    "                       colsample_bytree=0.1,\n",
    "                       #tree_method='gpu_hist',\n",
    "                       gpu_id=0)\n",
    "    \n",
    "    mdl.fit(X_train0, y_train0)\n",
    "\n",
    "    p = mdl.predict(X_train1)\n",
    "    \n",
    "    model_name_train1 = \"./preds_train1/xgb_depth_{}.pkl.z\".format(depth)\n",
    "    jb.dump(p, model_name_train1)\n",
    "\n",
    "    metric = spearmanr(y_train1.values,p).correlation\n",
    "\n",
    "    p = mdl.predict(X_val1)\n",
    "    \n",
    "    model_name_val1 = \"./preds_val1/xgb_depth_{}.pkl.z\".format(depth)\n",
    "    jb.dump(p, model_name_val1)\n",
    "\n",
    "    p = mdl.predict(X_test)\n",
    "    \n",
    "    model_name_test = \"./preds_test/xgb_depth_{}.pkl.z\".format(depth)\n",
    "    jb.dump(p, model_name_test)\n",
    "\n",
    "    print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6 0.042369021680469854 4959 0.5403948646972071 0.4312893194050142 0.02264096378677319\n",
      "1 6 0.049862055736004565 1005 0.32209931536865577 0.18208030173540174 0.018417548697186894\n",
      "2 9 0.010069021830487138 3706 0.9029575503750902 0.48627418710538006 0.028068046982699192\n",
      "3 9 0.03569924330828714 1068 0.5097448446778967 0.8536522589128892 0.027298761308558107\n",
      "4 8 0.009462071222525337 3771 0.5981320413049752 0.587800584692363 0.02873841936190224\n",
      "5 4 0.0037038260747930584 1825 0.23604723980548376 0.8767498171441294 0.03373588521467743\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "bad allocation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-903150d817bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mgen_xgb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-75-903150d817bd>\u001b[0m in \u001b[0;36mgen_xgb\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     17\u001b[0m                        seed=seed)\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mmdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m    595\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'eval_metric'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         self._Booster = train(params, train_dmatrix,\n\u001b[0m\u001b[0;32m    598\u001b[0m                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \"\"\"\n\u001b[1;32m--> 227\u001b[1;33m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    228\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;31m# Copy to serialise and unserialise booster to reset state and free\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;31m# training memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1167\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mcopied\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m         \"\"\"\n\u001b[1;32m-> 1169\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__copy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__copy__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__copy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1155\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__deepcopy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__deepcopy__\u001b[1;34m(self, _)\u001b[0m\n\u001b[0;32m   1157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[1;34m'''Return a copy of booster.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1159\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1161\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, cache, model_file)\u001b[0m\n\u001b[0;32m   1029\u001b[0m             \u001b[1;31m# We use the pickle interface for getting memory snapshot from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[1;31m# another model, and load the snapshot with this booster.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'handle'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'handle'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__getstate__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1067\u001b[0m             \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m             \u001b[0mcptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOINTER\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m             _check_call(_LIB.XGBoosterSerializeToBuffer(self.handle,\n\u001b[0m\u001b[0;32m   1070\u001b[0m                                                         \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m                                                         ctypes.byref(cptr)))\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \"\"\"\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXGBoostError\u001b[0m: bad allocation"
     ]
    }
   ],
   "source": [
    "def gen_xgb(seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    depth = np.random.randint(1,11)\n",
    "    learning_rate = np.random.uniform(1e-3, 5e-2)\n",
    "    n_estimators = np.random.randint(100, 5000)\n",
    "    colsample_bytree = np.random.uniform(0.05, 0.95)\n",
    "    subsample = np.random.uniform(0.05, 0.95)\n",
    "    \n",
    "    mdl = XGBRegressor(max_depth=depth,\n",
    "                       learning_rate=learning_rate,\n",
    "                       n_estimators=n_estimators,\n",
    "                       colsample_bytree=colsample_bytree, \n",
    "                       subsample=subsample,\n",
    "                       #tree_method='gpu_hist',\n",
    "                       gpu_id=0,\n",
    "                       seed=seed)\n",
    "    \n",
    "    mdl.fit(X_train0, y_train0)\n",
    "\n",
    "    p = mdl.predict(X_train1)\n",
    "    \n",
    "    model_name_train1 = \"./preds_train1/xgb_rhyper_{}.pkl.z\".format(seed)\n",
    "    jb.dump(p, model_name_train1)\n",
    "\n",
    "    metric = spearmanr(y_train1.values,p).correlation\n",
    "\n",
    "    p = mdl.predict(X_val1)\n",
    "    \n",
    "    model_name_val1 = \"./preds_val1/xgb_rhyper_{}.pkl.z\".format(seed)\n",
    "    jb.dump(p, model_name_val1)\n",
    "\n",
    "    p = mdl.predict(X_test)\n",
    "    \n",
    "    model_name_test = \"./preds_test/xgb_rhyper_{}.pkl.z\".format(seed)\n",
    "    jb.dump(p, model_name_test)\n",
    "\n",
    "    print(seed, depth, learning_rate, n_estimators, colsample_bytree, subsample, metric)\n",
    "    \n",
    "for seed in range(10):\n",
    "    gen_xgb(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerando Diversidade: Outras Maneiras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando Diversidade: Outras Maneiras - Classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesse exemplo nossa variável alvo pode ser entendida como uma classificação. Veja que ela é separada por 0.25, como se fosse classes. Então pode fazer sentido tratar como um problema de classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5 , 0.25, 0.75, 0.  , 1.  ])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train0.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2\n",
       "1         1\n",
       "2         1\n",
       "3         1\n",
       "4         3\n",
       "         ..\n",
       "142092    1\n",
       "142093    2\n",
       "142094    2\n",
       "142095    2\n",
       "142096    1\n",
       "Name: target, Length: 142097, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = {0.: 0, 0.25: 1, 0.5: 2, 0.75: 3, 1.:4}\n",
    "y_train0 = y_train0.map(labels)\n",
    "y_train0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_values = np.array([0., 0.25, 0.5, 0.75, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:18:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "0.03486939591299764\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "mdl = XGBClassifier(max_depth=5,\n",
    "                    learning_rate=0.01,\n",
    "                    n_estimators=2000,\n",
    "                    colsample_bytree=0.1,\n",
    "                    #tree_method='gpu_hist',\n",
    "                    gpu_id=0)\n",
    "\n",
    "mdl.fit(X_train0, y_train0)\n",
    "\n",
    "p = mdl.predict_proba(X_train1)\n",
    "\n",
    "p = p.dot(label_values)\n",
    "\n",
    "model_name_train1 = \"./preds_train1/xgb_classifier.pkl.z\"\n",
    "jb.dump(p, model_name_train1)\n",
    "\n",
    "metric = spearmanr(y_train1.values,p).correlation\n",
    "\n",
    "p = mdl.predict_proba(X_val1)\n",
    "p = p.dot(label_values)\n",
    "\n",
    "model_name_val1 = \"./preds_val1/xgb_classifier.pkl.z\"\n",
    "jb.dump(p, model_name_val1)\n",
    "\n",
    "p = mdl.predict_proba(X_test)\n",
    "p = p.dot(label_values)\n",
    "\n",
    "model_name_test = \"./preds_test/xgb_classifier.pkl.z\"\n",
    "jb.dump(p, model_name_test)\n",
    "\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando Diversidade: Outras Maneiras - Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = train_train_level0.groupby(\"era\").size().values\n",
    "X_train0, y_train0 = train_train_level0.filter(regex=r'feature', axis=1), train_train_level0['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n",
      "[10:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1190: Empty dataset at worker: 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'PytestTester' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-d69df87e110b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mp_ts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mera\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'era'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'era'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mera\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'PytestTester' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRanker\n",
    "\n",
    "mdl = XGBRanker(max_depth=5,\n",
    "                learning_rate=0.01,\n",
    "                n_estimators=2000,\n",
    "                colsample_bytree=0.1,\n",
    "                #tree_method='gpu_hist',\n",
    "                gpu_id=0)\n",
    "\n",
    "mdl.fit(X_train0, y_train0, group=groups)\n",
    "\n",
    "p_tr1 = np.zeros(X_train1.shape[0])\n",
    "\n",
    "for era in train_train_level1['era'].unique():\n",
    "    mask = train_train_level1['era'] == era\n",
    "    X = X_train1[mask]\n",
    "    p_tr1[mask] = mdl.predict(X)\n",
    "    \n",
    "model_name_train1 = \"./preds_train1/xgb_ranker_era.pkl.z\"\n",
    "jb.dump(p_tr1, model_name_train1)\n",
    "\n",
    "metric = spearmanr(y_train1.values,p_tr1).correlation\n",
    "    \n",
    "p_vl1 = np.zeros(X_val1.shape[0])\n",
    "\n",
    "for era in train_train_level1['era'].unique():\n",
    "    mask = train_valid_level1['era'] == era\n",
    "    X = X_val1[mask]\n",
    "    p_vl1[mask] = mdl.predict(X)\n",
    "    \n",
    "model_name_val1 = \"./preds_val1/xgb_ranker_era.pkl.z\"\n",
    "jb.dump(p_vl1, model_name_val1)\n",
    "    \n",
    "p_ts = np.zeros(X_test.shape[0])\n",
    "\n",
    "for era in test['era'].unique():\n",
    "    mask = test['era'] == era\n",
    "    X = X_test[mask]\n",
    "    p_ts[mask] = mdl.predict(X)\n",
    "    \n",
    "model_name_test = \"./preds_test/xgb_ranker_era.pkl.z\"\n",
    "jb.dump(p_ts, model_name_test)\n",
    "    \n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando Diversidade: Outras Maneiras - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.026992768255832234\n",
      "10 0.03165248747885688\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "bad allocation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-84-f216c3bfcd0c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \"\"\"\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mmdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m    595\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'eval_metric'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         self._Booster = train(params, train_dmatrix,\n\u001b[0m\u001b[0;32m    598\u001b[0m                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_num_boosting_rounds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[0mBooster\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m     \"\"\"\n\u001b[1;32m--> 227\u001b[1;33m     bst = _train_internal(params, dtrain,\n\u001b[0m\u001b[0;32m    228\u001b[0m                           \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                           \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;31m# Copy to serialise and unserialise booster to reset state and free\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;31m# training memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mcopy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1167\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mcopied\u001b[0m \u001b[0mbooster\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m         \"\"\"\n\u001b[1;32m-> 1169\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__copy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__copy__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__copy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1155\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__deepcopy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__deepcopy__\u001b[1;34m(self, _)\u001b[0m\n\u001b[0;32m   1157\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__deepcopy__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m         \u001b[1;34m'''Return a copy of booster.'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1159\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1161\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, cache, model_file)\u001b[0m\n\u001b[0;32m   1029\u001b[0m             \u001b[1;31m# We use the pickle interface for getting memory snapshot from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[1;31m# another model, and load the snapshot with this booster.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m             \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'handle'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'handle'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__getstate__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1067\u001b[0m             \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_bst_ulong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m             \u001b[0mcptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPOINTER\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m             _check_call(_LIB.XGBoosterSerializeToBuffer(self.handle,\n\u001b[0m\u001b[0;32m   1070\u001b[0m                                                         \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbyref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m                                                         ctypes.byref(cptr)))\n",
      "\u001b[1;32mR:\\Install\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \"\"\"\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXGBoostError\u001b[0m: bad allocation"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "\n",
    "#https://xgboost.readthedocs.io/en/latest/tutorials/rf.html#\n",
    "#subsample = 0.5, paper ~= bagging\n",
    "for depth in [5, 10, 20, 50, None]:\n",
    "    \n",
    "    mdl = XGBRegressor(max_depth=depth,\n",
    "                       learning_rate=1.,\n",
    "                       n_estimators=1,\n",
    "                       num_parallel_tree=1000,\n",
    "                       colsample_bytree=0.8,\n",
    "                       colsample_bynode=0.8,\n",
    "                       subsample=0.5,\n",
    "                       #tree_method='gpu_hist',\n",
    "                       gpu_id=0,\n",
    "                       random_state=0)\n",
    "    \n",
    "    \"\"\"\n",
    "    mdl = XGBRFRegressor(max_depth=depth,\n",
    "                         random_state=0,\n",
    "                         #tree_method='gpu_hist',\n",
    "                         gpu_id=0)\n",
    "    \"\"\"\n",
    "    \n",
    "    mdl.fit(X_train0, y_train0)\n",
    "\n",
    "    p = mdl.predict(X_train1)\n",
    "    \n",
    "    model_name_train1 = \"./preds_train1/xgb_rf_{}.pkl.z\".format(depth)\n",
    "    jb.dump(p, model_name_train1)\n",
    "\n",
    "    metric = spearmanr(y_train1.values,p).correlation\n",
    "\n",
    "    p = mdl.predict(X_val1)\n",
    "    \n",
    "    model_name_val1 = \"./preds_val1/xgb_rf_{}.pkl.z\".format(depth) \n",
    "    jb.dump(p, model_name_val1)\n",
    "\n",
    "    p = mdl.predict(X_test)\n",
    "    \n",
    "    model_name_test = \"./preds_test/xgb_rf_{}.pkl.z\".format(depth)\n",
    "    jb.dump(p, model_name_test)\n",
    "\n",
    "    print(depth, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando Diversidade: Outras Maneiras - Boosted Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui fazemos um boosting com Random Forest sendo usada em cada estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trees in [2, 5, 10]:\n",
    "    \n",
    "    mdl = XGBRegressor(max_depth=None,\n",
    "                       learning_rate=0.01,\n",
    "                       n_estimators=2000,\n",
    "                       colsample_bytree=0.1,\n",
    "                       subsample=1.,\n",
    "                       num_parallel_tree=trees,\n",
    "                       #tree_method='gpu_hist',\n",
    "                       gpu_id=0)\n",
    "    \n",
    "    mdl.fit(X_train0, y_train0)\n",
    "\n",
    "    p = mdl.predict(X_train1)\n",
    "    \n",
    "    model_name_train1 = \"./preds_train1/xgb_boosted_rf_{}.pkl.z\".format(trees)\n",
    "    jb.dump(p, model_name_train1)\n",
    "\n",
    "    metric = spearmanr(y_train1.values,p).correlation\n",
    "\n",
    "    p = mdl.predict(X_val1)\n",
    "    \n",
    "    model_name_val1 = \"./preds_val1/xgb_boosted_rf_{}.pkl.z\".format(trees)\n",
    "    jb.dump(p, model_name_val1)\n",
    "\n",
    "    p = mdl.predict(X_test)\n",
    "    model_name_test = \"./preds_test/xgb_boosted_rf_{}.pkl.z\".format(trees)\n",
    "    jb.dump(p, model_name_test)\n",
    "\n",
    "    print(trees, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleção Stack Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "preds_train1 = glob.glob(\"./preds_train1/*.pkl.z\")\n",
    "preds_val1 = glob.glob(\"./preds_val1/*.pkl.z\")\n",
    "\n",
    "df_train1 = []\n",
    "for p_name in preds_train1:\n",
    "    p = jb.load(p_name)\n",
    "    p_name = re.search(r\"train1/(.*)\\.pkl.z\", p_name).group(1)\n",
    "    p_df = pd.DataFrame(p, columns=[p_name])\n",
    "    df_train1.append(p_df)\n",
    "    \n",
    "    \n",
    "df_val1 = [] \n",
    "scores_val1 = dict()\n",
    "for p_name in preds_val1:\n",
    "    p = jb.load(p_name)\n",
    "    p_name = re.search(r\"val1/(.*)\\.pkl.z\", p_name).group(1)\n",
    "    p_df = pd.DataFrame(p, columns=[p_name])\n",
    "    df_val1.append(p_df)\n",
    "    scores_val1[p_name] = spearmanr(y_val1, p_df).correlation\n",
    "    \n",
    "    \n",
    "df_train1 = pd.concat(df_train1, axis=1)\n",
    "df_val1 = pd.concat(df_val1, axis=1)\n",
    "\n",
    "df_test = [] \n",
    "scores_test = dict()\n",
    "preds_test = glob.glob(\"./preds_test/*.pkl.z\")\n",
    "for p_name in preds_test:\n",
    "    p = jb.load(p_name)\n",
    "    p_name = re.search(r\"test/(.*)\\.pkl.z\", p_name).group(1)\n",
    "    p_df = pd.DataFrame(p, columns=[p_name])\n",
    "    df_test.append(p_df)\n",
    "    scores_test[p_name] = spearmanr(y_test, p_df).correlation\n",
    "    \n",
    "    \n",
    "df_test = pd.concat(df_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "df_train1.columns[~df_train1.columns.isin(df_val1.columns)]\n",
    "\n",
    "selected = ['xgb_classifier', \n",
    "            'xgb_feature_groups_feature_intelligence_feature_charisma_feature_dexterity',\n",
    "            'xgb_depth_1', 'xgb_rhyper_5']\n",
    "\n",
    "best_score = dist.max() + 1e-4 \n",
    "\n",
    "for col in df_train1.columns:\n",
    "    \n",
    "    if col not in selected:\n",
    "        \n",
    "        Xtr = df_train1[selected+[col]].copy()\n",
    "        Xval = df_val1[selected+[col]].copy()\n",
    "    \n",
    "        mdl = Ridge(alpha=1.)\n",
    "        mdl.fit(Xtr, y_train1)\n",
    "\n",
    "        p = mdl.predict(Xval)\n",
    "        c = spearmanr(y_val1, p).correlation\n",
    "        if c > best_score:\n",
    "            print(col, c)\n",
    "            best_score = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = []\n",
    "\n",
    "for seed in range(100):\n",
    "    \n",
    "    Xtr = df_train1[selected].copy()\n",
    "    Xval = df_val1[selected].copy()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    #print( Xtr.head())\n",
    "    Xtr['random'] = np.random.permutation(Xtr.iloc[:, -1].values)\n",
    "    Xval['random'] = np.random.permutation(Xval.iloc[:, -1].values)\n",
    "    \n",
    "    #print( Xtr.head())\n",
    "\n",
    "    mdl = Ridge(alpha=1.)\n",
    "    mdl.fit(Xtr, y_train1)\n",
    "\n",
    "    p = mdl.predict(Xval)\n",
    "    c = spearmanr(y_val1, p).correlation\n",
    "    #print(c)\n",
    "    dist.append(c)\n",
    "    \n",
    "dist = np.array(dist)\n",
    "dist.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = df_train1[selected].copy()\n",
    "mdl = Ridge(alpha=1.)\n",
    "mdl.fit(Xtr, y_train1)\n",
    "\n",
    "p = mdl.predict(df_test[selected])\n",
    "\n",
    "spearmanr(y_test, p).correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Test com XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = ['xgb_classifier', 'xgb_feature_groups_feature_charisma_feature_dexterity_feature_wisdom', \n",
    "            'xgb_feature_groups_feature_intelligence_feature_strength', 'lgbm_feature_groups_feature_constitution',\n",
    "           'mlp_109_39_0.0009911332752617498', 'xgb_feature_groups_feature_intelligence_feature_charisma_feature_constitution',\n",
    "           'lgbm_feature_groups_feature_strength_feature_dexterity', 'lgbm_feature_groups_feature_charisma_feature_dexterity_feature_constitution',\n",
    "           'xgb_feature_groups_feature_intelligence_feature_wisdom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "best_score = dist.max() + 1e-4\n",
    "#best_score = 0\n",
    "\n",
    "for col in df_train1.columns:\n",
    "    if col not in selected:\n",
    "        Xtr = df_train1[selected+[col]].copy()\n",
    "        Xval = df_val1[selected+[col]].copy()\n",
    "    \n",
    "    \n",
    "        mdl = XGBRegressor(n_estimators=1000, learning_rate=0.01, max_depth=1, random_state=0, colsample_bytree=0.1, subsample=0.25, \n",
    "                   tree_method='gpu_hist', gpu_id=0)\n",
    "        mdl.fit(Xtr, y_train1)\n",
    "\n",
    "        p = mdl.predict(Xval)\n",
    "        c = spearmanr(y_val1, p).correlation\n",
    "        if c > best_score:\n",
    "            print(col, c)\n",
    "            best_score = c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = []\n",
    "for seed in range(20):\n",
    "    Xtr = df_train1[selected].copy()\n",
    "    Xval = df_val1[selected].copy()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    #print( Xtr.head())\n",
    "    Xtr['random'] = np.random.permutation(Xtr.iloc[:, -1].values)\n",
    "    Xval['random'] = np.random.permutation(Xval.iloc[:, -1].values)\n",
    "    \n",
    "    #print( Xtr.head())\n",
    "\n",
    "    mdl = XGBRegressor(n_estimators=1000, learning_rate=0.01, max_depth=1, random_state=0, colsample_bytree=0.1, subsample=0.25, \n",
    "                   tree_method='gpu_hist', gpu_id=0)\n",
    "    mdl.fit(Xtr, y_train1)\n",
    "\n",
    "    p = mdl.predict(Xval)\n",
    "    c = spearmanr(y_val1, p).correlation\n",
    "    #print(c)\n",
    "    dist.append(c)\n",
    "dist = np.array(dist)\n",
    "dist.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Final com Média Ponderada de 2 Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ver como combinar os dois modelos no segundo nível da Stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selected cols (xgb + ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "selected_xgb = ['xgb_classifier', 'xgb_feature_groups_feature_charisma_feature_dexterity_feature_wisdom', \n",
    "            'xgb_feature_groups_feature_intelligence_feature_strength', 'lgbm_feature_groups_feature_constitution',\n",
    "           'mlp_109_39_0.0009911332752617498', 'xgb_feature_groups_feature_intelligence_feature_charisma_feature_constitution',\n",
    "           'lgbm_feature_groups_feature_strength_feature_dexterity', 'lgbm_feature_groups_feature_charisma_feature_dexterity_feature_constitution',\n",
    "           'xgb_feature_groups_feature_intelligence_feature_wisdom']\n",
    "\n",
    "Xtr = df_train1[selected_xgb].copy()\n",
    "Xval = df_val1[selected_xgb].copy()\n",
    "mdl = XGBRegressor(n_estimators=1000, learning_rate=0.01, max_depth=1, random_state=0, colsample_bytree=0.1, subsample=0.25, \n",
    "                   tree_method='gpu_hist', gpu_id=0)\n",
    "mdl.fit(Xtr, y_train1)\n",
    "\n",
    "p_xgb_s = mdl.predict(Xval)\n",
    "c = spearmanr(y_val1, p_xgb_s).correlation\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "selected_ridge  = ['xgb_classifier', 'xgb_feature_groups_feature_intelligence_feature_charisma_feature_dexterity', 'xgb_depth_1', 'xgb_rhyper_5']\n",
    "\n",
    "Xtr = df_train1[selected_ridge].copy()\n",
    "Xval = df_val1[selected_ridge].copy()\n",
    "mdl = Ridge(alpha=1.)\n",
    "mdl.fit(Xtr, y_train1)\n",
    "\n",
    "p_ridge_s = mdl.predict(Xval)\n",
    "c = spearmanr(y_val1, p_ridge_s).correlation\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar diferentes combinações de peso para achar a melhor configuração\n",
    "p = 0.3*p_ridge_s + 0.7*p_xgb_s\n",
    "c = spearmanr(y_val1, p).correlation\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todas as cols (xgb + ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = XGBRegressor(n_estimators=1000, learning_rate=0.01, max_depth=1, random_state=0, colsample_bytree=0.1, subsample=0.25, \n",
    "                   tree_method='gpu_hist', gpu_id=0)\n",
    "mdl.fit(df_train1, y_train1)\n",
    "p_xgb_t = mdl.predict(df_val1)\n",
    "c = spearmanr(y_val1, p_xgb_t).correlation\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = Ridge(alpha=1.)\n",
    "mdl.fit(df_train1, y_train1)\n",
    "p_ridge_t = mdl.predict(df_val1)\n",
    "c = spearmanr(y_val1, p_ridge_t).correlation\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.2*p_ridge_t + 0.8*p_xgb_t\n",
    "c = spearmanr(y_val1, p).correlation\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com a selação de features o resultado foi levemente melhor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = df_train1[selected_xgb].copy()\n",
    "mdl = XGBRegressor(n_estimators=1000, learning_rate=0.01, max_depth=1, random_state=0, colsample_bytree=0.1, subsample=0.25, \n",
    "                   tree_method='gpu_hist', gpu_id=0)\n",
    "mdl.fit(Xtr, y_train1)\n",
    "\n",
    "p_xgb_test = mdl.predict(df_test[selected_xgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "Xtr = df_train1[selected_ridge].copy()\n",
    "mdl = Ridge(alpha=1.)\n",
    "mdl.fit(Xtr, y_train1)\n",
    "\n",
    "p_ridge_test = mdl.predict(df_test[selected_ridge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.3*p_ridge_test + 0.7*p_xgb_test\n",
    "c = spearmanr(y_test, p).correlation\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No teste perdemos um pouco a acertividade do modelo. Caiu para 0,20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E o feature set completo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = df_train1\n",
    "mdl = XGBRegressor(n_estimators=1000, learning_rate=0.01, max_depth=1, random_state=0, colsample_bytree=0.1, subsample=0.25, \n",
    "                   tree_method='gpu_hist', gpu_id=0)\n",
    "mdl.fit(Xtr, y_train1)\n",
    "\n",
    "p_xgb_test = mdl.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = df_train1\n",
    "mdl = Ridge(alpha=1.)\n",
    "mdl.fit(Xtr, y_train1)\n",
    "\n",
    "p_ridge_test = mdl.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.2*p_ridge_test + 0.8*p_xgb_test\n",
    "c = spearmanr(y_test, p).correlation\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Melhora val\", 0.04476200295534109 / 0.04290319376803716 - 1 )\n",
    "print(\"Melhor test\", 0.024410911110673422 / 0.024535468740518818 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
